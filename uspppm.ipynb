{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Patent Phrase to Phrase Matching \n",
    "Help Identify Similar Phrases in U.S. Patents  \n",
    "\n",
    "[U.S. Patent Phrase to Phrase Matching | Kaggle](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching)\n",
    "\n",
    "### Introduction to the competition \n",
    "In this competition, the model will be trained on a new **semantic similarity dataset** to extract relevant information by matching **key phrases in patent documents**. During patent search and review, determining semantic similarity between phrases is crucial to determining whether an invention has been previously described. \n",
    "+ Type of competition: This competition belongs to Deep Learning/Natural Language Processing, so recommended model or library :Bert/DeBERTa/ELECTRA\n",
    "+ Problem data: Contestants are presented with pairs of phrases (an anchor and a target) and asked to rate how similar they are on a scale from 0 to 1. The officially provided training set has about 36,000 pairs of phrases, and the test set has about 12,000 pairs.\n",
    "+ Evaluation criteria: **Pearson correlation coefficient**. [U.S. Patent Phrase to Phrase Matching | Kaggle](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/overview/evaluation) \n",
    "\n",
    "### Preparation done before the competition  \n",
    "+ Course for Deep Learning.[Neural Networks and Deep Learning | Coursera](https://www.coursera.org/learn/neural-networks-deep-learning)  \n",
    "  My license for the course.[Neural Networks and Deep Learning | Coursera](https://www.coursera.org/account/accomplishments/verify/RWVTZ62KDKR5) \n",
    "+ An article about EDA(Exploring Data Analysis) on Kaggle. [EDA | Kaggle](https://www.kaggle.com/code/remekkinas/eda-and-feature-engineering)\n",
    " \n",
    "### Data declaration \n",
    "In this dataset, you are presented with pairs of phrases (an anchor and a target) and asked to rate how similar they are on a scale from 0(not similar at all) to 1(the same). This task differs from the standard semantic similarity task in that similarity here is scored in the context of the patent, specifically its **CPC classification**, which indicates the subject matter covered by the patent. For example, while the phrases \"bird\" and \"Cape Cod\" might have low semantic similarity in normal language, their meaning similarity would be closer if considered in the context of \"house\". \n",
    "\n",
    "The official document is as follows:[Neural Networks and Deep Learning | Coursera](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data) \n",
    "\n",
    "+ train.csv — — A training set containing phrases, contexts and their similarity scores.\n",
    "+ test.csv — — Same construct as train.csv without scores.\n",
    "+ sample_submission.csv — — A sample submission file in the correct format.\n",
    "+ train.csv/test.csv fields: \n",
    "     - id — — A unique identifier for a pair of phrases.\n",
    "     - anchor — — The first phrase.\n",
    "     - target — — The second phrase.\n",
    "     - context — — **CPC classification** (version 2021.05), which represents topics to be scored for similarity.\n",
    "     - score — — Similarity ranges from 0 to 1, with step equals 0.25:\n",
    "          * 1.0 - Very close match. This is typically an exact match except possibly for differences in conjugation, quantity (e.g. singular vs. plural), and addition or removal of stopwords (e.g. “the”, “and”, “or”).\n",
    "          * 0.75 - Close synonym, e.g. “mobile phone” vs. “cellphone”. This also includes abbreviations, e.g. \"TCP\" -> \"transmission control protocol\".\n",
    "          * 0.5 - Synonyms which don’t have the same meaning (same function, same properties). This includes broad-narrow (hyponym) and narrow-broad (hypernym) matches.\n",
    "          * 0.25 - Somewhat related, e.g. the two phrases are in the same high level domain but are not synonyms. This also includes antonyms.\n",
    "          * 0.0 - Unrelated. \n",
    "\n",
    "### Solution idea \n",
    "\n",
    "#### Data processing \n",
    "+ We first introduce the title of each patent code in the external **CPC** file as the title text. Then we **groupby** the anchor and context to get the aggregated targets list (GP_targets). On this basis, the training text \"anchor [SEP] target [SEP] title [SEP] gp_targets\" is generated. \n",
    "+ The data is split into training and validation sets using **Groupkfold**, and the group column is anchor. \n",
    "\n",
    "#### Model selection/structure\n",
    "+ We choose the multi-model fusion of Bert For Patent + DeBERTa + ELECTRA + Funnel-Transformer\n",
    "+ The model structure uses the above model as backbone, adding a Linear layer and Sigmoid. \n",
    "\n",
    "| Model  | seq_length  | CV score | PB score |\n",
    "| :----: | :----: | :----: | :----: |\n",
    "| deberta-v3-large | 200 | 0.844 | 0.842 |\n",
    "| electra-large | 200 | 0.832 | 0.833 |\n",
    "| funnel-large | 200 | 0.824 | 0.825 |\n",
    "| bert-for-patents | 200 | 0.824 | 0.824 |\n",
    "| **ensemble** |  | **0.855** | **0.868** |    \n",
    "\n",
    "#### Datasets\n",
    "+ Official datasets (train.csv, test.csv) [U.S. Patent Phrase to Phrase Matching | Kaggle](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data)\n",
    "+ [CPC data](https://www.kaggle.com/datasets/yasufuminakama/cpc-data)\n",
    "+ [deberta](https://www.kaggle.com/datasets/xhlulu/deberta)\n",
    "+ [deberta-v3-base](https://www.kaggle.com/datasets/jonathanchan/deberta-v3-base)\n",
    "+ [deberta-v3-large](https://www.kaggle.com/datasets/jonathanchan/deberta-v3-large)\n",
    "+ [electra](https://www.kaggle.com/datasets/xhlulu/electra)\n",
    "+ [funnel-large](https://www.kaggle.com/datasets/goldenlock/funnel-large)\n",
    "+ [bert-for-patent](https://www.kaggle.com/datasets/ksork6s4/bert-for-patents) \n",
    "\n",
    "#### Other useful methods \n",
    "+ Loss function: MSEloss\n",
    "+ Optimizer: AdamW\n",
    "+ Scheduler: CosineAnnealingWarmRestarts\n",
    "+ FMG confrontation training  \n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel, AdamW, AutoTokenizer\n",
    "import sys\n",
    "import scipy\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "import scipy.stats\n",
    "\n",
    "class CFG:\n",
    "    result_dir = '/home/huaxuechun/workspace/pppm' # result dir\n",
    "    data_dir = '/home/huaxuechun/workspace/us-patent-phrase-to-phrase-matching' # data dir\n",
    "    k_folds = 5 # k folds\n",
    "    n_jobs = 5 # n_jobs\n",
    "    seed = 42 # random seed\n",
    "    device = torch.cuda.is_available() # use cuda\n",
    "    print_freq = 100 # print frequency\n",
    "    \n",
    "    model_name = 'bert-for-patents' # model name  # electra-large / deberta-v3-large / funnel-large / bert-for-patents\n",
    "    base_epoch = 5 # epoch\n",
    "    batch_size = 32 # batch size\n",
    "    lr = 1e-5 # learning rate\n",
    "    seq_length = 200 # sequence length\n",
    "    max_grad_norm = 1 # gradient clipping\n",
    "    \n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "\n",
    "class KFold(object):\n",
    "    \"\"\"\n",
    "    Group split by group_col\n",
    "    \"\"\"\n",
    "    def __init__(self, k_folds=10, flag_name='fold_flag'):\n",
    "        self.k_folds = k_folds # k folds\n",
    "        self.flag_name = flag_name # fold_flag\n",
    "\n",
    "    def group_split(self, train_df, group_col): \n",
    "        group_value = list(set(train_df[group_col])) # group value\n",
    "        group_value.sort() # sort\n",
    "        fold_flag = [i % self.k_folds for i in range(len(group_value))] # fold_flag\n",
    "        np.random.shuffle(fold_flag) # shuffle\n",
    "        train_df = train_df.merge(pd.DataFrame({group_col: group_value, self.flag_name: fold_flag}), how='left', on=group_col) # merge\n",
    "        return train_df\n",
    "\n",
    "def get_data():\n",
    "    train_df = pd.read_csv(CFG.data_dir + '/train.csv') # train data\n",
    "    train_df = KFold(CFG.k_folds).group_split(train_df, group_col='anchor') # kfold group split\n",
    "    titles = get_cpc_texts() # cpc texts\n",
    "    train_df = get_text(train_df, titles) # # train data get text\n",
    "    test_df = pd.read_csv(CFG.data_dir + '/test.csv') # test data\n",
    "    test_df['score'], test_df['fold_flag'] = 0, -1 # test fill score and fold_flag\n",
    "    test_df = get_text(test_df, titles) # # test data get text\n",
    "    print(train_df.shape, test_df.shape) # print shape\n",
    "    return train_df, test_df # return train and test data\n",
    "\n",
    "def get_text(df, titles):\n",
    "    df['anchor'] = df['anchor'].apply(lambda x:x.lower()) # anchor lower\n",
    "    df['target'] = df['target'].apply(lambda x:x.lower()) # target lower\n",
    "    # title\n",
    "    df['title'] = df['context'].map(titles)\n",
    "    df['title'] = df['title'].apply(lambda x:x.lower().replace(';', '').replace('  ',' ').strip())\n",
    "\n",
    "    df = df.join(df.groupby(['anchor', 'context']).target.agg(list).rename('gp_targets'), on=['anchor', 'context']) # group by anchor and context and get target_list\n",
    "    df['gp_targets'] = df.apply(lambda x: ', '.join([i for i in x['gp_targets'] if i != x['target']]), axis=1) # get gp_targets\n",
    "    df['text'] = df['anchor'] + '[SEP]' + df['target'] + '[SEP]'  + df['title'] + '[SEP]'  + df['gp_targets'] # anchor [SEP] target [SEP] title [SEP] gp_targets\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cpc_texts():\n",
    "    '''\n",
    "    get cpc texts\n",
    "    '''\n",
    "    # get cpc codes\n",
    "    contexts = []  \n",
    "    pattern = '[A-Z]\\d+'\n",
    "    for file_name in os.listdir(f'{CFG.data_dir}/cpc-data/CPCSchemeXML202105'):\n",
    "        result = re.findall(pattern, file_name)\n",
    "        if result:\n",
    "            contexts.append(result)\n",
    "    contexts = sorted(set(sum(contexts, []))) # all unique cpc codes\n",
    "    # like ['A01', 'A21', 'A22', 'A23', 'A24', 'A41', 'A42', 'A43', 'A44', 'A45']\n",
    "    \n",
    "    results = {}\n",
    "    for cpc in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'Y']:\n",
    "        with open(f'{CFG.data_dir}/cpc-data/CPCTitleList202202/cpc-section-{cpc}_20220201.txt') as f:\n",
    "            s = f.read()\n",
    "        # 总目录及其text 如 \"A\t\tHUMAN NECESSITIES\"\n",
    "        pattern = f'{cpc}\\t\\t.+' \n",
    "        result = re.findall(pattern, s)\n",
    "        pattern = \"^\"+pattern[:-2]\n",
    "        cpc_result = re.sub(pattern, \"\", result[0]) # 获取描述，如 'HUMAN NECESSITIES'\n",
    "\n",
    "        for context in [c for c in contexts if c[0] == cpc]:\n",
    "            pattern = f'{context}\\t\\t.+'\n",
    "            result = re.findall(pattern, s) # cpc code及其text 如 'A01\\t\\tAGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTING; TRAPPING; FISHING'\n",
    "            pattern = \"^\"+pattern[:-2]\n",
    "            results[context] = cpc_result + \". \" + re.sub(pattern, \"\", result[0]) # 生成字典 like {'A01': 'HUMAN NECESSITIES. AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTING; TRAPPING; FISHING'}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentDataset(Dataset):\n",
    "    def __init__(self, meta_data: pd.DataFrame, tokenizer, fold: int = -1, mode='train'):\n",
    "        self.meta_data = meta_data.copy() # meta_data\n",
    "        self.meta_data.reset_index(drop=True, inplace=True) # reset index\n",
    "        if mode == 'train':\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] != fold].copy() # train data\n",
    "        elif mode == 'valid':\n",
    "            self.meta_data = self.meta_data[self.meta_data['fold_flag'] == fold].copy() # valid data\n",
    "        elif mode == 'test':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(mode)\n",
    "        self.meta_data.reset_index(drop=True, inplace=True) # reset index\n",
    "        self.seq_length = CFG.seq_length # seq_length\n",
    "        if tokenizer.sep_token != '[SEP]': \n",
    "            self.meta_data['text'] = self.meta_data['text'].apply(lambda x:x.replace('[SEP]', tokenizer.sep_token )) # replace [SEP] to tokenizer.sep_token\n",
    "        self.text = self.meta_data['text'].values # text\n",
    "        self.target = self.meta_data['score'].values # target\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        seq = self.text[index] # seq\n",
    "        target = self.target[index] # target\n",
    "        encoded = self.tokenizer.encode_plus(\n",
    "            text=seq, # text\n",
    "            add_special_tokens=True, # add_special_tokens \n",
    "            max_length=self.seq_length, # max_length\n",
    "            padding='max_length', # padding\n",
    "            return_attention_mask=True, # return_attention_mask\n",
    "            return_tensors='pt', # return_tensors\n",
    "            truncation=True # truncation\n",
    "        )\n",
    "        input_ids = encoded['input_ids'][0] # input_ids\n",
    "        attention_mask = encoded['attention_mask'][0] # attention_mask\n",
    "\n",
    "        return input_ids, attention_mask, np.array(target, dtype=np.float32) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.meta_data) # len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatentModel(nn.Module):\n",
    "    def __init__(self, name, num_classes=1, pretrained=True):\n",
    "        super(PatentModel, self).__init__()\n",
    "        self.config = AutoConfig.from_pretrained(name) # config\n",
    "        self.attention_probs_dropout_prob=0. # attention_probs_dropout_prob\n",
    "        self.hidden_dropout_prob=0. # hidden_dropout_prob\n",
    "        if pretrained:\n",
    "            self.encoder = AutoModel.from_pretrained(name, config=self.config) \n",
    "        else:\n",
    "            self.encoder = AutoModel.from_config(self.config)\n",
    "        in_dim = self.encoder.config.hidden_size # get hidden_size\n",
    "        self.last_fc = nn.Linear(in_dim, num_classes) # last_fc\n",
    "        torch.nn.init.normal_(self.last_fc.weight, std=0.02) # init last_fc\n",
    "        self.sig = nn.Sigmoid() # Sigmoid\n",
    "\n",
    "    def forward(self, seq, seq_mask):\n",
    "        x = self.encoder(seq, attention_mask=seq_mask)[\"last_hidden_state\"] # forward                       # torch.Size([32, 200, 1024])\n",
    "        x = torch.sum(x * seq_mask.unsqueeze(-1), dim=1) / torch.sum(seq_mask, dim=1).unsqueeze(-1) # mean  # torch.Size([32, 1024])\n",
    "        out = self.last_fc(x) # last_fc                                                                     # torch.Size([32, 1])\n",
    "        out = self.sig(out) # Sigmoid                                                                       # torch.Size([32, 1])\n",
    "        out = torch.squeeze(out)                                                                            # torch.Size([32])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_test_df(df, tokenizer, batch_size):\n",
    "    # input ids lengths list \n",
    "    input_lengths = [] \n",
    "    for text in df['text'].fillna(\"\").values:\n",
    "        length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n",
    "        input_lengths.append(length)\n",
    "    df['input_lengths'] = input_lengths\n",
    "    length_sorted_idx = np.argsort([-l for l in input_lengths])\n",
    "\n",
    "    # sort dataframe by lengths\n",
    "    sort_df = df.iloc[length_sorted_idx]\n",
    "    # calc max_len per batch\n",
    "    sorted_input_length = sort_df['input_lengths'].values # \n",
    "    batch_max_length = np.zeros_like(sorted_input_length) # zeros_like \n",
    "    # every batch\n",
    "    for i in range((len(sorted_input_length)//batch_size)+1):\n",
    "        batch_max_length[i*batch_size:(i+1)*batch_size] = np.max(sorted_input_length[i*batch_size:(i+1)*batch_size]) # max input length in every batch\n",
    "    sort_df['batch_max_length'] = batch_max_length\n",
    "    return sort_df, length_sorted_idx\n",
    "    \n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset() # reset\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0. \n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def save_model(model, save_path, model_name):\n",
    "    '''\n",
    "    save model\n",
    "    '''\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    filename = os.path.join(save_path, model_name + '.pth.tar')\n",
    "    torch.save({'state_dict': model.state_dict(), }, filename)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    \"\"\"\n",
    "    Handles PyTorch x Numpy seeding issues.\n",
    "\n",
    "    Args:\n",
    "        worker_id (int): Id of the worker.\n",
    "    \"\"\"\n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    '''\n",
    "    MSELoss \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss = (inputs - targets) ** 2\n",
    "        loss = loss.mean()\n",
    "        loss = torch.sqrt(loss)\n",
    "        return loss\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    return scipy.stats.pearsonr(y_true, y_pred)[0] # pearsonr\n",
    "\n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name and param.grad is not None:\n",
    "                # print(name, param)\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = epsilon * param.grad / max(norm, 0.001)\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name and param.grad is not None:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "def get_model_path(model_name):\n",
    "    '''\n",
    "    get model path\n",
    "    '''\n",
    "    res = CFG.result_dir\n",
    "    if model_name in ['electra-base', 'electra-large']:\n",
    "        res += '/electra/' + model_name.split('-')[1] + '-discriminator'\n",
    "    elif model_name == 'deberta-v3-large':\n",
    "        res += '/deberta-v3-large/'\n",
    "    elif model_name == 'funnel-large':\n",
    "        res += '/funnel-large/'\n",
    "    elif model_name == 'bert-for-patents':\n",
    "        res += '/bert-for-patents/'\n",
    "    else:\n",
    "        raise ValueError(model_name)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, scheduler, optimizer, epoch, is_adversial=False):\n",
    "    # training\n",
    "    batch_time = AverageMeter() # batch time\n",
    "    losses = AverageMeter() # loss\n",
    "    # switch to train mode\n",
    "    model.train() # train mode\n",
    "    fgm = FGM(model) if is_adversial else None # fgm\n",
    "    start = time.time()\n",
    "    for i, batch_data in enumerate(train_loader):\n",
    "        # optimizer.zero_grad()\n",
    "        if CFG.device:\n",
    "            batch_data = (t.cuda() for t in batch_data)\n",
    "        seq, seq_mask, target = batch_data\n",
    "        # print(seq.shape,seq_mask.shape,target.shape)\n",
    "        output = model(seq, seq_mask)\n",
    "        # print(seq.shape,seq_mask.shape,target.shape,output.shape)\n",
    "        loss = criterion(output, target) # loss\n",
    "        losses.update(loss.item())\n",
    "        loss = loss\n",
    "        loss.backward()\n",
    "        if is_adversial:\n",
    "            # 对抗训练\n",
    "            fgm.attack()  # 在embedding上添加对抗扰动\n",
    "            output = model(seq, seq_mask) # 模型输出\n",
    "            loss_adv = criterion(output, target)  # 计算loss\n",
    "            loss_adv.backward()  # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            fgm.restore()  # 恢复embedding参数\n",
    "        if CFG.max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm) # 梯度裁剪\n",
    "        optimizer.step() # 更新参数\n",
    "        optimizer.zero_grad() # 清空梯度\n",
    "\n",
    "        batch_time.update(time.time() - start) # update batch time\n",
    "        start = time.time() # update start time\n",
    "        if i % CFG.print_freq == 0: \n",
    "            print('Epoch: [{0}][{1}/{2}], Loss {loss:.4f}\\n'.format(epoch, i, len(train_loader), loss=loss.item())) # print info\n",
    "    return losses.avg, batch_time.sum \n",
    "\n",
    "\n",
    "def validate(model, valid_loader, tokenizer):\n",
    "    model.eval() # eval mode\n",
    "    y_pred = [] # y_pred\n",
    "    for i, batch_data in enumerate(valid_loader):\n",
    "        if CFG.device:\n",
    "            batch_data = (t.cuda() for t in batch_data) # cuda\n",
    "        seq, seq_mask, target = batch_data # batch_data\n",
    "        output = model(seq, seq_mask) # model output\n",
    "        y_pred.append(output.detach().cpu().numpy()) # y_pred\n",
    "    y_pred = np.concatenate(y_pred) # y_pred\n",
    "    score = get_score(valid_loader.dataset.target, y_pred) # score\n",
    "    # scoring\n",
    "    return y_pred, score # y_pred, score\n",
    "\n",
    "\n",
    "print('------------------------------------------------------Training-------------------------------------------------\\n')\n",
    "warnings.filterwarnings('ignore') # ignore warnings\n",
    "model_dir = os.path.join(CFG.result_dir, 'models') # model dir\n",
    "os.makedirs(model_dir, exist_ok=True) # make dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "print('>> data_processing...\\n')\n",
    "\n",
    "# load data\n",
    "train_df, test_df = get_data() \n",
    "\n",
    "oof_prediction = np.zeros((len(train_df))) # oof prediction\n",
    "eval_loss = [] # eval loss\n",
    "\n",
    "for fold in range(CFG.k_folds):\n",
    "    model = PatentModel(get_model_path(CFG.model_name), pretrained=True) # load model\n",
    "    model.zero_grad() # zero grad\n",
    "    model = model.cuda() # cuda\n",
    "    tokenizer = AutoTokenizer.from_pretrained(get_model_path(CFG.model_name)) # load tokenizer\n",
    "    train_model_filename = os.path.join(model_dir, CFG.model_name + '_fold{}.pth.tar'.format(fold)) # train model filename\n",
    "\n",
    "    if fold == 0:\n",
    "        col_lengths = [] # col lengths\n",
    "        for text in train_df['text'].fillna(\"\").values: # get col lengths\n",
    "            length = len(tokenizer(text, add_special_tokens=True)['input_ids'])\n",
    "            col_lengths.append(length)\n",
    "        print(f'text max(lengths): {max(col_lengths)} {np.percentile(col_lengths, 95)}')\n",
    "\n",
    "    train_dataset = PatentDataset(train_df, tokenizer, fold, mode='train') # train dataset\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=CFG.batch_size, num_workers=CFG.n_jobs, pin_memory=True, worker_init_fn=worker_init_fn) # train loader\n",
    "    valid_dataset = PatentDataset(train_df, tokenizer, fold, mode='valid') # valid dataset \n",
    "    valid_loader = DataLoader(valid_dataset, shuffle=False, batch_size=CFG.batch_size * 4, num_workers=CFG.n_jobs, pin_memory=True) # valid loader\n",
    "    criterion = MSELoss() # criterion\n",
    "\n",
    "    best_score = -1\n",
    "    patience_cnt = 0\n",
    "    is_improved = True\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, betas=(0.9, 0.999), eps=1e-6, weight_decay=0) # optimizer\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.base_epoch, eta_min=CFG.lr / 5) # scheduler\n",
    "    for epoch in range(CFG.base_epoch):\n",
    "        scheduler.step(epoch=epoch) # scheduler\n",
    "        print('Fold: [{0}] Epoch: [{1}], lr:[{2}]\\n'.format(fold, epoch, optimizer.param_groups[0]['lr'])) # print Fold, Epoch, lr\n",
    "        is_adversial = True # 对抗训练\n",
    "        train_loss, train_batch_time = train(model, train_loader, criterion, scheduler, optimizer, epoch, is_adversial) # train\n",
    "        print('Epoch avg loss: {0}, Epoch cost time:{1} min\\n'.format(train_loss, train_batch_time / 60)) # print Epoch avg loss, Epoch cost time\n",
    "        with torch.no_grad(): \n",
    "            y_pred, score = validate(model, valid_loader, tokenizer) # validate\n",
    "            print('Epoch score: {0}\\n'.format(score)) # print Epoch score\n",
    "            if score > best_score:\n",
    "                best_score, best_epoch = score, epoch # best_score, best_epoch\n",
    "                oof_prediction[np.where(train_df['fold_flag'] == fold)] = y_pred.copy() # oof prediction\n",
    "                save_model(model, model_dir, '{}_fold{}_seed{}'.format(CFG.model_name, fold, CFG.seed)) # save model\n",
    "                print('********Best Epoch: [{0}], Best Score:{1}********\\n'.format(best_epoch, best_score)) # print Best Epoch, Best Score\n",
    "            else:\n",
    "                is_improved = False # is_improved\n",
    "                patience_cnt += 1 # patience_cnt\n",
    "\n",
    "    eval_loss.append(best_score) # eval loss\n",
    "    del model, y_pred\n",
    "    _ = gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "print('CV mean:{} std:{}.'.format(np.mean(eval_loss), np.std(eval_loss))) # CV mean, std\n",
    "print('detail:{}'.format(np.round(eval_loss, 4))) # detail\n",
    "np.save(os.path.join(CFG.result_dir, CFG.model_name + '_oof.npy'), oof_prediction) # save oof prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "from transformers import BertTokenizer, RobertaTokenizerFast, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "CFG.batch_size = 32 # batch size\n",
    "CFG.n_jobs = 4 # n_jobs\n",
    "CFG.seq_length = 512 # seq_length\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # TOKENIZERS_PARALLELISM\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    # switch to evaluate mode\n",
    "    model.eval() # model\n",
    "    y_pred = []\n",
    "    for i, batch_data in enumerate(data_loader): # 载入每个batch的数据\n",
    "        batch_data = (t.cuda() for t in batch_data)\n",
    "        seq, seq_mask, _ = batch_data # seq, seq_mask, target\n",
    "        outputs = model(seq, seq_mask).detach().cpu().numpy() # outputs\n",
    "        y_pred.append(outputs)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def get_preds(my_df, my_loader, my_model, model_path, model_name=''):\n",
    "    my_model.load_state_dict(torch.load(model_path)['state_dict']) # 载入模型\n",
    "    my_model = my_model.cuda()\n",
    "    with torch.no_grad():\n",
    "        y_pred = predict(my_model, my_loader) # 获得y_pred\n",
    "    return y_pred\n",
    "\n",
    "train_df, test_df = get_data() # 获得训练集和测试集\n",
    "ensemble_weight = [0.2, 0.6, 0.1, 0.1] \n",
    "    \n",
    "print('>> predicting...\\n')\n",
    "start = time.time()\n",
    "# -------------------- Model 1 --------------------\n",
    "model_name = 'bert-for-patents' # model_name\n",
    "tokenizer_path = get_model_path(model_name) # get_model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) # tokenizer\n",
    "\n",
    "sort_df, length_sorted_idx = get_sorted_test_df(test_df.copy(), tokenizer, batch_size=CFG.batch_size) # sort_df, length_sorted_idx\n",
    "test_dataset = PatentDatasetV2(sort_df, tokenizer) # test_dataset\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=CFG.batch_size, num_workers=CFG.n_jobs, drop_last=False, pin_memory=True) # test_loader\n",
    "\n",
    "res1 = []\n",
    "folds = range(CFG.k_folds)\n",
    "for fold in folds:\n",
    "    model = PatentModel(get_model_path(model_name), pretrained=False) # model\n",
    "    model_path = '/home/huaxuechun/workspace/pppm/models/{}_fold{}_seed{}.pth.tar'.format(model_name, fold, 42) # model_path\n",
    "    print(model_path)\n",
    "    y_preds = get_preds(test_df, test_loader, model, model_path) # y_preds\n",
    "    y_preds = y_preds[np.argsort(length_sorted_idx)] # y_preds\n",
    "    res1.append(y_preds)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "res1 = np.mean(res1, axis=0)\n",
    "\n",
    "# -------------------- Model 2 --------------------\n",
    "model_name = 'deberta-v3-large' # model_name\n",
    "tokenizer_path = get_model_path(model_name) # get_model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) # tokenizer\n",
    "\n",
    "sort_df, length_sorted_idx = get_sorted_test_df(test_df.copy(), tokenizer, batch_size=CFG.batch_size) # sort_df, length_sorted_idx\n",
    "test_dataset = PatentDatasetV2(sort_df, tokenizer) # test_dataset\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=CFG.batch_size, num_workers=CFG.n_jobs, drop_last=False, pin_memory=True) # test_loader\n",
    "res2 = []\n",
    "folds = range(CFG.k_folds)\n",
    "for fold in folds:\n",
    "    model = PatentModel(get_model_path(model_name), pretrained=False) # model\n",
    "    model_path = '/home/huaxuechun/workspace/pppm/models/{}_fold{}_seed{}.pth.tar'.format(model_name, fold, 42) # model_path\n",
    "    print(model_path)\n",
    "    y_preds = get_preds(test_df, test_loader, model, model_path) # y_preds\n",
    "    y_preds = y_preds[np.argsort(length_sorted_idx)] # y_preds\n",
    "    res2.append(y_preds)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "res2 = np.mean(res2, axis=0)\n",
    "\n",
    "# -------------------- Model 3 --------------------\n",
    "model_name = 'electra-large' # model_name\n",
    "tokenizer_path = get_model_path(model_name)# get_model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) # tokenizer\n",
    "\n",
    "sort_df, length_sorted_idx = get_sorted_test_df(test_df.copy(), tokenizer, batch_size=CFG.batch_size) # sort_df, length_sorted_idx\n",
    "test_dataset = PatentDatasetV2(sort_df, tokenizer) # test_dataset\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=CFG.batch_size, num_workers=CFG.n_jobs, drop_last=False, pin_memory=True) # test_loader\n",
    "res3 = []\n",
    "folds = range(CFG.k_folds)\n",
    "for fold in folds:\n",
    "    model = PatentModel(get_model_path(model_name), pretrained=False) # model\n",
    "    model_path = '/home/huaxuechun/workspace/pppm/models/{}_fold{}_seed{}.pth.tar'.format(model_name, fold, 42) # model_path\n",
    "    print(model_path)\n",
    "    y_preds = get_preds(test_df, test_loader, model, model_path) # y_preds\n",
    "    y_preds = y_preds[np.argsort(length_sorted_idx)] # y_preds\n",
    "    res3.append(y_preds)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "res3 = np.mean(res3, axis=0)\n",
    "\n",
    "# -------------------- Model 4 --------------------\n",
    "model_name = 'funnel-large' # model_name\n",
    "tokenizer_path = get_model_path(model_name) # get_model_path\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) # tokenizer\n",
    "\n",
    "sort_df, length_sorted_idx = get_sorted_test_df(test_df.copy(), tokenizer, batch_size=CFG.batch_size) # sort_df, length_sorted_idx\n",
    "test_dataset = PatentDatasetV2(sort_df, tokenizer) # test_dataset\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=CFG.batch_size, num_workers=CFG.n_jobs, drop_last=False, pin_memory=True) # test_loader\n",
    "res4 = []\n",
    "folds = range(CFG.k_folds)\n",
    "for fold in folds:\n",
    "    model = PatentModel(get_model_path(model_name), pretrained=False) # model\n",
    "    model_path = '/home/huaxuechun/workspace/pppm/models/{}_fold{}_seed{}.pth.tar'.format(model_name, fold, 42) # model_path\n",
    "    print(model_path)\n",
    "    y_preds = get_preds(test_df, test_loader, model, model_path) # y_preds\n",
    "    y_preds = y_preds[np.argsort(length_sorted_idx)] # y_preds\n",
    "    res4.append(y_preds)\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "res4 = np.mean(res4, axis=0)\n",
    "\n",
    "# ensemble\n",
    "res = [res1,res2,res3,res4]\n",
    "for i in range(len(res)):\n",
    "    res[i] = (res[i] - res[i].mean())/res[i].std()\n",
    "test_df['score'] = np.sum([res[i] * ensemble_weight[i] for i in range(len(res))], axis=0)\n",
    "test_df['score'] = (test_df['score'] - test_df['score'].mean()) /test_df['score'].std()\n",
    "\n",
    "# get submission\n",
    "print(test_df.shape)\n",
    "test_df[['id', 'score']].to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Process \n",
    "+ Use **deberta-base baseline**, PB score: 0.789\n",
    "+ Introduce the **title text** from the CPC file to the training set, PB score: 0.795\n",
    "+ Groupby is performed on Anchor and Context to obtain the aggregated targets list (gp_targets), PB score: 0.810\n",
    "+ Exchange  **deberta-base baseline** to **deberta-v3-large**, PB score: 0.841\n",
    "+ Use **electra-large**, PB score: 0.831\n",
    "+ Use **funnel-large**, PB score: 0.825\n",
    "+ Use **bert-for-patent**, PB score: 0.822\n",
    "+ Ensemble, PB score: 0.847\n",
    "+ Introduce FMG confrontation training, PB score: 0.849\n",
    "+ Adjust parameter like learning rate, seq_length, fold_numbers, PB score: 0.855"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6293ff032c3dd15ec8bc52ae1ca561ee1103fe3a702597ce12604decd90e38bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
